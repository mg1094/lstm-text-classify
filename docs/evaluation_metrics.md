# 模型评估指标详解

## 🎯 概述

在机器学习的分类任务中，准确评估模型性能是至关重要的。本文档详细介绍了项目中使用的各种评估指标。

## 📊 核心评估指标

### 1. 准确率 (Accuracy)

**定义**：正确预测样本数占总样本数的比例

**公式**：
```
Accuracy = (TP + TN) / (TP + TN + FP + FN)
```

**特点**：
- ✅ 最直观的性能指标
- ❌ 在类别不平衡时可能产生误导

**适用场景**：类别分布相对均衡的数据集

### 2. 精确率 (Precision)

**定义**：预测为正类的样本中，实际为正类的比例

**公式**：
```
Precision = TP / (TP + FP)
```

**特点**：
- 关注预测为正类的准确性
- 回答"预测为正类的样本中，有多少是真正的正类？"

**适用场景**：关注误报代价高的场景（如垃圾邮件检测）

### 3. 召回率 (Recall)

**定义**：实际为正类的样本中，被正确预测的比例

**公式**：
```
Recall = TP / (TP + FN)
```

**特点**：
- 关注正类的识别完整性
- 回答"实际的正类样本中，有多少被成功识别？"

**适用场景**：关注漏报代价高的场景（如疾病诊断）

### 4. F1分数 (F1-Score)

**定义**：精确率和召回率的调和平均值

**公式**：
```
F1 = 2 × (Precision × Recall) / (Precision + Recall)
```

**特点**：
- 平衡考虑精确率和召回率
- 在类别不平衡时比准确率更可靠

**适用场景**：需要在精确率和召回率之间取得平衡

## 🔄 指标关系图

```
实际情况 vs 预测结果的关系：

        预测
      正  负
实 正  TP  FN  ← Recall = TP/(TP+FN)
际 负  FP  TN
      ↑
   Precision = TP/(TP+FP)
```

## 📈 指标选择指南

### 根据业务场景选择

| 场景 | 重点指标 | 原因 |
|------|----------|------|
| 垃圾邮件检测 | Precision | 避免误删重要邮件 |
| 疾病诊断 | Recall | 避免漏诊 |
| 推荐系统 | F1-Score | 平衡准确性和覆盖率 |
| 欺诈检测 | Recall | 避免漏检欺诈行为 |

### 数据特征考虑

| 数据特征 | 推荐指标 | 说明 |
|----------|----------|------|
| 类别平衡 | Accuracy | 各类别样本数量相近 |
| 类别不平衡 | F1-Score | 避免被多数类误导 |
| 少数类重要 | Recall | 确保少数类被识别 |
| 误报代价高 | Precision | 减少假阳性 |

## 🎨 可视化工具

### 1. 混淆矩阵
- 详细展示各类别的分类情况
- 直观发现分类错误模式
- [详细说明](confusion_matrix_guide.md)

### 2. ROC曲线
- 展示真正率vs假正率的关系
- 评估不同阈值下的性能

### 3. PR曲线
- 展示精确率vs召回率的关系
- 在类别不平衡时比ROC更有意义

## 💡 实际应用建议

### 1. 多指标综合评估
不要仅依赖单一指标，而应该综合考虑：
```python
# 项目中的评估示例
metrics = {
    'accuracy': 86.5,
    'precision': 87.6,
    'recall': 85.0,
    'f1': 86.3
}
```

### 2. 考虑业务成本
- 分析误分类的业务成本
- 根据成本调整模型阈值
- 优化对业务最重要的指标

### 3. 持续监控
- 定期检查模型性能
- 关注指标趋势变化
- 及时发现性能退化

## 🔧 项目中的实现

### 指标计算
```python
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

# 计算详细指标
precision, recall, f1, _ = precision_recall_fscore_support(
    all_targets, all_preds, average='weighted'
)

metrics = {
    'accuracy': accuracy_score(all_targets, all_preds) * 100,
    'precision': precision * 100,
    'recall': recall * 100,
    'f1': f1 * 100
}
```

### 结果输出
- 训练过程中实时显示
- 保存到`results.json`文件
- 生成可视化图表

## 📋 常见问题

### Q1: 为什么准确率很高，但模型表现不好？
A: 可能是类别不平衡导致的。假设100个样本中有95个负类，5个正类，即使模型总是预测负类，准确率也有95%，但实际上模型没有学到任何有用信息。

### Q2: 精确率和召回率哪个更重要？
A: 取决于具体应用场景：
- 如果误报代价高（如误诊），优先提高精确率
- 如果漏报代价高（如漏诊），优先提高召回率
- 大多数情况下，使用F1分数平衡两者

### Q3: 如何提高模型的评估指标？
A: 
1. 增加高质量的训练数据
2. 改进特征工程
3. 调整模型结构和超参数
4. 使用数据增强技术
5. 处理类别不平衡问题

---

**总结**：选择合适的评估指标对于正确评估模型性能至关重要。在实际项目中，应该根据业务需求和数据特征，综合使用多个指标来全面评估模型效果。